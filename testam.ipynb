{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(problem='tsp', graph_size=20, batch_size=512, epoch_size=1280000, val_size=10000, val_dataset=None, model='attention', embedding_dim=128, hidden_dim=128, n_encode_layers=3, tanh_clipping=10.0, normalization='batch', lr_model=0.0001, lr_critic=0.0001, lr_decay=1.0, eval_only=False, n_epochs=100, seed=1234, max_grad_norm=1.0, no_cuda=False, exp_beta=0.8, baseline=None, bl_alpha=0.05, bl_warmup_epochs=0, eval_batch_size=1024, checkpoint_encoder=False, shrink_size=None, data_distribution=None, log_step=50, log_dir='logs', run_name='run_20231215T113614', output_dir='outputs', epoch_start=0, checkpoint_epochs=1, load_path=None, resume=None, no_tensorboard=False, no_progress_bar=False, use_cuda=True, save_dir='outputs/tsp_20/run_20231215T113614')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\n",
    "## parameters \n",
    "from options import get_options\n",
    "\n",
    "opts = get_options()\n",
    "opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate and load data\n",
    "import numpy as np\n",
    "def generate_tsp_data(dataset_size, tsp_size):\n",
    "    return np.random.uniform(size=(dataset_size, tsp_size, 2)).tolist()\n",
    "\n",
    "dataset_size = 10000\n",
    "graph_size = 20\n",
    "\n",
    "dataset = generate_tsp_data(dataset_size, graph_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## graph embedding\n",
    "\n",
    "import torch.nn as nn \n",
    "from graph_encoder import GraphAttentionEncoder\n",
    "node_dim = 2  # x, y\n",
    "embedding_dim = 128\n",
    "init_embed = nn.Linear(node_dim, embedding_dim)\n",
    "\n",
    "# embedder = GraphAttentionEncoder(\n",
    "#             n_heads=n_heads,\n",
    "#             embed_dim=embedding_dim,\n",
    "#             n_layers=self.n_encode_layers,\n",
    "#             normalization=normalization\n",
    "#         )\n",
    "\n",
    "# embeddings, _ = embedder(init_embed(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_encoder import GraphAttentionEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from options import get_options\n",
    "\n",
    "\n",
    "class AttentionModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 problem,\n",
    "                 n_encode_layers=2,\n",
    "                 tanh_clipping=10.,\n",
    "                 mask_inner=True,\n",
    "                 mask_logits=True,\n",
    "                 normalization='batch',\n",
    "                 n_heads=8,\n",
    "                 checkpoint_encoder=False,\n",
    "                 shrink_size=None):\n",
    "        super(AttentionModel, self).__init__()\n",
    "        node_dim = 2  # x, y\n",
    "        self.embedder = GraphAttentionEncoder(\n",
    "            n_heads=n_heads,\n",
    "            embed_dim=embedding_dim,\n",
    "            n_layers=self.n_encode_layers,\n",
    "            normalization=normalization\n",
    "        )\n",
    "\n",
    "        self.init_embed = nn.Linear(node_dim, embedding_dim)\n",
    "\n",
    "\n",
    "    def _init_embed(self, input):\n",
    "\n",
    "        if self.is_vrp or self.is_orienteering or self.is_pctsp:\n",
    "            if self.is_vrp:\n",
    "                features = ('demand', )\n",
    "            elif self.is_orienteering:\n",
    "                features = ('prize', )\n",
    "            else:\n",
    "                assert self.is_pctsp\n",
    "                features = ('deterministic_prize', 'penalty')\n",
    "            return torch.cat(\n",
    "                (\n",
    "                    self.init_embed_depot(input['depot'])[:, None, :],\n",
    "                    self.init_embed(torch.cat((\n",
    "                        input['loc'],\n",
    "                        *(input[feat][:, :, None] for feat in features)\n",
    "                    ), -1))\n",
    "                ),\n",
    "                1\n",
    "            )\n",
    "        # TSP\n",
    "        return self.init_embed(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
