{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Implementation of [Neural Combinatorial Optimization with Reinforcement Learning](https://arxiv.org/pdf/1611.09940.pdf)\n",
    "\n",
    "Thanks for [neural-combinatorial-rl-pytorch](https://github.com/pemami4911/neural-combinatorial-rl-pytorch) and [combinatorial optimization with DL/RL](https://github.com/higgsfield/np-hard-deep-reinforcement-learning). Their work helps me understand the theory in paper and fix debugs in my codes. I also used some of their codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "USE_CUDA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pointer Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "* One layer of LSTM with (embedding_dim, hidden_dim)\n",
    "* Initialize h, c with zeros\n",
    "* Output\n",
    "    * a sequence of latent memory states $\\{enc_i\\}_{i=1}^n$, where $\\{enc_i\\} \\in \\mathbb{R}^d$, d is embedding dimension\n",
    "    * h: last layer hidden states, h\\[-1\\] is the hidden state in the last cell\n",
    "    * c: last layer cell states, c\\[-1\\] is the cell state in the last cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        # use default number of layers is 1\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, embedding_x):\n",
    "        '''\n",
    "        Args:\n",
    "        embedding_x (tensor): shape (sequence length, batch, input_dim)\n",
    "        hidden (tensor): shape (num_layer * num_direction, batch_size, hidden_dim)\n",
    "        \n",
    "        Return:\n",
    "        output (tensor): (sequence length, batch_size, hidden_dim)\n",
    "        next_hidden (tensor): (num_layer * num_direction, batch_size, hidden_dim)\n",
    "        '''\n",
    "        output, next_hidden = self.lstm(embedding_x)\n",
    "        return output, next_hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention and Glimpse (Pointing and Attending Mechanism)\n",
    "\n",
    "* Input:\n",
    "    * a query vector from decoder, $q = dec_i \\in \\mathbb{R}^d$\n",
    "    * a set of reference vector from encoder output, $ref = \\{enc_i, \\dots, enc_k\\}$， k could be the sequence length\n",
    "* Pointing mechanism:\n",
    "    $$u_i = \\begin{cases} v^T \\cdot \\tanh (W_{ref} \\cdot r_t + W_q \\cdot q) \\hspace{0.2cm} \\text{if} \\hspace{0.2cm} i \\neq \\pi(j) \\hspace{0.2cm} \\text{for all} \\hspace{0.2cm} j < i \\\\\n",
    "    -\\infty\\end{cases}  \\hspace{0.2cm} \\text{for}  \\hspace{0.2cm} i = 1, 2, \\dots k$$\n",
    "    where $W_{ref}, W_q \\in \\mathbb{R}^{d \\times d}$ and an attention vector $v \\in \\mathbb{R}^d$. In practice, the $reference$ input is (batch size $\\times$ embedding dimension $\\times$ sequence length), we can use convolution weights with shape (embedding dimensions, embedding dimensions) to address 3D data. $v$ initializes with normal distribution between $\\left[-\\frac{1}{\\sqrt{d}}, \\frac{1}{\\sqrt{d}}\\right]$ In the paper, it is $\\left[-0.08 (-\\frac{1}{\\sqrt{128}}), 0.08\\right]$. \n",
    "    $$A(ref, q; W_{ref}, W_q, v) = softmax(u)$$\n",
    "    * Improving exploration\n",
    "        * Softmax temperature\n",
    "          $$A(ref, q; W_{ref}, W_q, v) = softmax(u / T)$$\n",
    "          where $T$ is the temperature hyperparameter set to T = 1 during training. $T>1$, $A$ is more smooth, so that is prevent the model from being overconfident\n",
    "        * Logit clipping (used here)\n",
    "          $$A(ref, q; W_{ref}, W_q, v) = softmax(C \\tanh(u))$$\n",
    "          where $C$ is the hyperparameter that controls the range of the logits. The paper uses $C = 10$\n",
    "* Attending mechanism\n",
    "    $$p = A(ref, q; W_{ref}, W_q, v)$$\n",
    "    $$G(ref, q; W_{ref}^g, W_q^g, v^g) = \\sum_{i = 1}^k r_i p_i$$\n",
    "    where $G$ is the glimpse function. It can be applied multiple times on the same reference set $ref$:\n",
    "    $$g_0 = q$$\n",
    "    $$g_l = G(ref, g_{l-1}; W_{ref}^q, W_q^g, v^g)$$\n",
    "    The paper says more than once glimpse lead to barely improve thre results. Thus, we choose $\\textbf{num_glimpse} = 1$. Glimpse function is implemented in the decoder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, C = 10, use_logit_clip = True, use_cuda = USE_CUDA):\n",
    "        super().__init__()\n",
    "        self.w_q = nn.Linear(dim, dim) # for query (batch_size, d)\n",
    "        self.w_ref = nn.Conv1d(dim, dim, 1, 1) # for reference (batch_size, d, k)\n",
    "        V = torch.Tensor(dim).float() # to trainable parameters\n",
    "        if use_cuda:\n",
    "            V = V.cuda()\n",
    "        # initialize v by uniform in almost (-0.08, 0.08) from the paper\n",
    "        self.v = nn.Parameter(V)\n",
    "        self.v.data.uniform_(- 1. / math.sqrt(dim), 1. / math.sqrt(dim))\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.C = C\n",
    "        self.dim = dim\n",
    "        self.use_logit_clip = use_logit_clip\n",
    "    \n",
    "    def forward(self, encoder_output, query):\n",
    "        '''\n",
    "        Args:\n",
    "        k is sequence length\n",
    "        encoder_output (tensor): shape is (k, batch_size, dim) from encoder_output\n",
    "        query (tensor): shape is (batch_size, dim)\n",
    "        \n",
    "        Return:\n",
    "        ref (tensor): shape is (batch_size, dim, k)\n",
    "        logit (tensor): probability with shape (batch_size, k)\n",
    "        '''\n",
    "        batch_size = query.size(0)\n",
    "        encoder_output = encoder_output.permute(1, 2, 0)\n",
    "        # make sure ref shape is (batch_size, dim, k)\n",
    "        \n",
    "        q = self.w_q(query).unsqueeze(2)\n",
    "        ref = self.w_ref(encoder_output) # batch_size, hidden_dim, k\n",
    "        k = ref.size(2)\n",
    "        expanded_q = q.repeat(1, 1, k)\n",
    "        expanded_v = self.v.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        # batch matrix multiply (batch_size, sequence_length)\n",
    "        u = torch.bmm(expanded_v, self.tanh(ref + expanded_q)).squeeze(1)\n",
    "        \n",
    "        if self.use_logit_clip:\n",
    "            logit = self.C * self.tanh(u)\n",
    "        else:\n",
    "            logit = u\n",
    "            \n",
    "        # return ref for glimpse\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mask already search index \n",
    "def mask_logit(logit, mask, prev_idx):\n",
    "    '''\n",
    "    mask logit probability if they are in previous indice\n",
    "    Args:\n",
    "    logit (tensor): from attention output with shape (batch_size, sequence length)\n",
    "    mask (tensor or None): selected mask\n",
    "    prev_idx (tensor): None or previous retrieved indice (batch_size, )\n",
    "    \n",
    "    Return:\n",
    "    logit (tensor): same shape with input, but mask elements\n",
    "    '''\n",
    "    mask_copy = mask.clone()\n",
    "    \n",
    "    batch_size = logit.size(0)\n",
    "    if prev_idx is not None:\n",
    "        mask_copy[[b for b in range(batch_size)], prev_idx.data] = 1\n",
    "        logit[mask_copy] = -np.inf\n",
    "    \n",
    "    return logit, mask_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "It also maintains its latent memory states $\\{dec_i\\}_{i=1}^n$, where $dec_i \\in \\mathbb{R}^d$.\n",
    "* Input:\n",
    "    * encoder last cell output: hidden state and cell state\n",
    "    * decoder input initializes with normal distribution between $\\left[-\\frac{1}{\\sqrt{d}}, \\frac{1}{\\sqrt{d}}\\right]$. It should be trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, sequence_length, \n",
    "                 decoder_type = \"sampling\", num_glimpse = 1, use_cuda = USE_CUDA):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_glimpse = num_glimpse\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # lstm cell weights\n",
    "        self.lstm_cell = nn.LSTMCell(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # pointer and glimpse\n",
    "        self.pointer = Attention(hidden_dim, use_cuda = use_cuda)\n",
    "        self.glimpse = Attention(hidden_dim, use_logit_clip=False, use_cuda = use_cuda)\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "        self.decoder_type = decoder_type\n",
    "    \n",
    "    def forward(self, decoder_input, embedding_x, hidden, encoder_output):\n",
    "        '''\n",
    "        Args:\n",
    "        decoder_input (tensor): (batch_size, embedding_dim)\n",
    "        embedding_x (tensor): (k, batch_size, embedding_dim)\n",
    "        hidden (tuple): (h, c), initially, \n",
    "                    (encoder_output[-1], encoder_output[-1]), (batch_size, hidden_dim)\n",
    "        encoder_output (tensor): encoder output, shape is (k, batch_size, embedding_dim)\n",
    "        \n",
    "        Return:\n",
    "        prob_list (list): list of probability through the sequence \n",
    "        index_list (list): list of indice\n",
    "        hidden (tuple): last layer hidden state and cell state\n",
    "        '''\n",
    "        batch_size = decoder_input.size(0)\n",
    "        seq_len = embedding_x.size(0)\n",
    "        # save result\n",
    "        prob_list = []\n",
    "        index_list = []\n",
    "        mask = None\n",
    "        mask = torch.zeros(batch_size, seq_len).byte()\n",
    "        if self.use_cuda:\n",
    "            mask = mask.cuda()\n",
    "        prev_idx = None\n",
    "        \n",
    "        embedding_x = embedding_x.permute(1, 0, 2) # to (batch_size, seq_len, embedding_dim)\n",
    "        ref = encoder_output.permute(1, 2, 0) # to (batch_size, embedding_dim, seq_len)\n",
    "        for i in range(self.sequence_length):\n",
    "            h, c = self.lstm_cell(decoder_input, hidden)\n",
    "            hidden = (h, c)\n",
    "            g_l = h  # (batch_size, hidden_dim)\n",
    "            for _ in range(self.num_glimpse):\n",
    "                logit = self.glimpse(encoder_output, g_l)\n",
    "                logit, mask = mask_logit(logit, mask, prev_idx)\n",
    "                p = self.softmax(logit) # (batch_size, seq_len)\n",
    "                # ref (batch_size, hidden_dim, seq_len)\n",
    "                g_l = torch.bmm(ref, p.unsqueeze(2)).squeeze(2) # (batch_size, hidden_dim)\n",
    "            logit = self.pointer(encoder_output, g_l)\n",
    "            logit_mask, mask = mask_logit(logit, mask, prev_idx)\n",
    "\n",
    "            if self.decoder_type == \"greedy\": # for validation\n",
    "                probs = self.softmax(logit_mask) # batch_size, k\n",
    "                prev_idx, decoder_input = greedy(probs, embedding_x)\n",
    "            elif self.decoder_type == \"sampling\": # for training\n",
    "                probs = self.softmax(softmax_temperature(logit_mask, T = 1.) )# batch_size, k\n",
    "                prev_idx, decoder_input = sampling(probs, embedding_x, index_list)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            # record previous index \n",
    "            index_list.append(prev_idx)\n",
    "            # record probability (like lstm output)\n",
    "            prob_list.append(probs)\n",
    "        return prob_list, index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def greedy(probs, embedding_x):\n",
    "    '''\n",
    "    greedy search and return the index with \n",
    "    the biggest probability used in validation set\n",
    "    \n",
    "    Args:  \n",
    "    probs (tensor): (batch_size, k)\n",
    "    embedding_x (tensor): (k, batch_size, embedding_dim)\n",
    "    \n",
    "    Return:\n",
    "    -- new_decoder_input (tensor): selected embedding x with shape (batch_size, embedding_dim)\n",
    "    as new decoder_input\n",
    "    -- idx (tensor): selected idx tensor （k,)\n",
    "    '''\n",
    "    batch_size = probs.size(0)\n",
    "    idx = torch.argmax(probs, dim = 1).long()\n",
    "    new_decoder_input = embedding_x[[x for x in range(batch_size)], idx.data, :]\n",
    "    return idx, new_decoder_input\n",
    "\n",
    "def sampling(probs, embedding_x, prev_idx):\n",
    "    '''\n",
    "    sampling indice from probability used in train and validation is ok\n",
    "    \n",
    "    Args:\n",
    "    probs (tensor): (batch_size, k)\n",
    "    embedding_x (tensor): (k, batch_size, embedding_dim)\n",
    "    prev_idx (list): list of previous index (batch_size, 1), should be LongTensor\n",
    "    \n",
    "    Return:\n",
    "    -- new_decoder_input (tensor): selected embedding x with shape (batch_size, embedding_dim)\n",
    "    as new decoder_input\n",
    "    -- idx (tensor): selected idx tensor （k,)\n",
    "    '''\n",
    "    batch_size = probs.size(0)\n",
    "    idx = probs.multinomial(1).squeeze(1).long()\n",
    "    \n",
    "    def is_exist(idx, prev_idx):\n",
    "        for old_idx in prev_idx:\n",
    "            if old_idx.eq(idx).data.any():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    while is_exist(idx, prev_idx):\n",
    "        idx = probs.multinomial(1).squeeze(1).long()  \n",
    "    \n",
    "    return idx, embedding_x[[x for x in range(batch_size)], idx.data, :]\n",
    "\n",
    "def softmax_temperature(logit_mask, T = 2.):\n",
    "    '''\n",
    "    Implement softmax temperature strategy\n",
    "    \n",
    "    Args:\n",
    "    logit_mask (batch_size, seq_len)\n",
    "    T (float): temperature\n",
    "    '''\n",
    "    return logit_mask / T\n",
    "\n",
    "def active_search(self):\n",
    "    \"TO DO\"    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pointer Network Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we should embed the raw input for the input of the network. Here I spent much time fixing bugs. The embedding for encoders should be shared for a sequence length of LSTM cells inputs, otherwise it will take much time to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, use_cuda = USE_CUDA):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.embedding = nn.Parameter(torch.FloatTensor(input_size, embedding_size)) \n",
    "        self.embedding.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        Args:\n",
    "        inputs (batch_size, seq_len, input_size)\n",
    "        \n",
    "        Return:\n",
    "        embedded (batch_size, seq_len, hidden_size)\n",
    "        '''\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len    = inputs.size(1)\n",
    "        embedding = self.embedding.repeat(batch_size, 1, 1) # batch_size, input_size, embedding_size\n",
    "        embedded = []\n",
    "        for i in range(seq_len):\n",
    "            # batch multiplication should be same dimensions, so, unsqueeze 1\n",
    "            embedded.append(torch.bmm(inputs[:, i, :].unsqueeze(1).float(), embedding))\n",
    "        embedded = torch.cat(embedded, 1)\n",
    "        return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PointerNet(nn.Module): \n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, \\\n",
    "                 seq_length, \\\n",
    "                 decoder_type = \"sampling\", num_glimpse = 1, use_cuda = USE_CUDA):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define encoder and decoder\n",
    "        self.encoder = Encoder(embedding_dim, hidden_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_dim, \\\n",
    "                               seq_length, \\\n",
    "                               decoder_type = decoder_type, \\\n",
    "                               num_glimpse = num_glimpse, use_cuda = use_cuda)\n",
    "\n",
    "        decoder_input = torch.Tensor(embedding_dim).float()\n",
    "        self.decoder_input = nn.Parameter(decoder_input) # trainable, default require grad is true\n",
    "        self.decoder_input.data.uniform_(-1 / math.sqrt(embedding_dim), 1 / math.sqrt(embedding_dim))\n",
    "        \n",
    "        # embedding\n",
    "        self.embedding = Embedding(input_dim, embedding_dim, use_cuda = use_cuda)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        propagate through the network\n",
    "        Args:\n",
    "        x (tensor): (batch_size, k, input_dim), like in paper, embedding dim = d\n",
    "        \n",
    "        Return:\n",
    "        - prob_list (list): length is max length of decoder, \n",
    "                with the element shape (batch_size, sequence length), \n",
    "        - index_list (list): length is max length of decoder, \n",
    "                with the element shape (batch_size,)\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "        embedding_x = self.embedding(x).permute(1, 0, 2)\n",
    "        encoder_output, (encoder_ht, encoder_ct) = self.encoder(embedding_x)\n",
    "        \n",
    "        # last layer output h, c as decoder initial state\n",
    "        hidden = (encoder_ht.squeeze(0), encoder_ct.squeeze(0)) # (batch_size, hidden_dim)\n",
    "\n",
    "        decoder_input = self.decoder_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "        prob_list, index_list = self.decoder(decoder_input, embedding_x, hidden, encoder_output)\n",
    "        \n",
    "        return prob_list, index_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic Network for TSP\n",
    "\n",
    "Map an input sequence $s$ into a baseline prediction $b_{\\theta_v}(s)$, because the objective\n",
    "$$\\mathcal{L}(\\theta_v) = \\frac{1}{B} \\sum_{i=1}^B \\left\\|b_{\\theta_v}(s) - L(\\pi_i | s_i) \\right\\|_2^2$$\n",
    "\n",
    "* an LSTM encoder: same as the encode in pointer network\n",
    "* an LSTM process block: performs P steps of computation over the hidden state $h$, update by glimpsing the memory state\n",
    "* a 2-layer ReLU neural network decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, process_iters, use_logit_clip = False, use_cuda = USE_CUDA):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(embedding_dim, hidden_dim)   \n",
    "        self.process_block = Attention(hidden_dim, use_logit_clip=use_logit_clip, \\\n",
    "                                       use_cuda = use_cuda) # output is (batch_size, hidden_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(hidden_dim, hidden_dim),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(hidden_dim, 1)\n",
    "                        )\n",
    "        self.process_iters = process_iters\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.embedding = Embedding(input_dim, embedding_dim, use_cuda = use_cuda)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "        x (tensor): with shape (k, batch_size, embedding_dim)\n",
    "        \n",
    "        Return:\n",
    "        output (tensor): (batch_size, 1)\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "        embedding_x = self.embedding(x).permute(1, 0, 2)\n",
    "        encoder_output, (encoder_ht, encoder_ct) = self.encoder(embedding_x)\n",
    "        ref = encoder_output.permute(1, 2, 0) # to (batch_size, embedding_dim, seq_len)\n",
    "        g_l = encoder_ht.squeeze(0)\n",
    "        for p in range(self.process_iters):\n",
    "            logit = self.process_block(encoder_output, g_l)\n",
    "            p = self.softmax(logit) # (batch_size, k)\n",
    "            g_l = torch.bmm(ref, p.unsqueeze(2)).squeeze(2)\n",
    "        output = self.decoder(g_l)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinatorial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, seq_len, batch_size = 128, process_iters = 3, use_cuda = USE_CUDA):\n",
    "        super().__init__()\n",
    "        self.pointer_net = PointerNet(embedding_dim, hidden_dim, seq_len, use_cuda = use_cuda)\n",
    "        self.critic = Critic(embedding_dim, hidden_dim, process_iters, use_cuda = use_cuda)\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "            \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "        x (batch_size, seq_len, input_dim)\n",
    "        '''\n",
    "        prob_list, index_list = self.pointer_net(x)\n",
    "        b = self.critic(x)\n",
    "        \n",
    "        pi = []\n",
    "        probs = []\n",
    "        for i, index in enumerate(index_list):\n",
    "            pi_ = x[[j for j in range(self.batch_size)], index.data, :]\n",
    "            pi.append(pi_)\n",
    "            prob_ = prob_list[i]\n",
    "            prob_ = prob_[[j for j in range(self.batch_size)], index.data]\n",
    "            probs.append(prob_)\n",
    "        \n",
    "        L = tour_length(pi)\n",
    "        log_probs = 0\n",
    "        for prob in probs:\n",
    "            log_prob = torch.log(prob)\n",
    "            log_probs += log_prob\n",
    "        \n",
    "        return L, log_probs, pi, index_list, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic and Moving Average Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate tour length\n",
    "\n",
    "$$L(\\pi | s) = \\left\\| x_{\\pi(n)} - x_{\\pi(1)} \\right\\|_2 + \\sum_{i=1}^{n-1} \\left\\|x_{\\pi(i)} - x_{\\pi(i+1)} \\right\\|_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define tour length function as reward\n",
    "def tour_length(pi, use_cuda = USE_CUDA):\n",
    "    '''\n",
    "    calculate the total length of the tour\n",
    "    Args:\n",
    "    pi (list): length is sequence length, \n",
    "                the element shape is (batch_size, point_size)\n",
    "    Return:\n",
    "    tour_len (tensor): (batch_size, 1)\n",
    "    '''\n",
    "    \n",
    "    n = len(pi)\n",
    "    batch_size = pi[0].size(0)\n",
    "    tour_len = Variable(torch.zeros(batch_size))\n",
    "    \n",
    "    if use_cuda:\n",
    "        tour_len = tour_len.cuda()\n",
    "        \n",
    "    for i in range(n-1):\n",
    "        tour_len += torch.norm(pi[i+1] - pi[i], p = 2, dim = 1)\n",
    "    tour_len += torch.norm(pi[n-1] - pi[0], p = 2, dim = 1)\n",
    "    return tour_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "input_dim = 2\n",
    "embedding_dim = 128\n",
    "batch_size = 128\n",
    "hidden_dim = 128\n",
    "process_iters = 3\n",
    "tsp_num = 20\n",
    "train_size = 1000000\n",
    "validation_size = 10000\n",
    "lr = 1e-4\n",
    "beta = 0.9\n",
    "num_glimpse = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size:  1000000\n",
      "validation dataset size:  10000\n"
     ]
    }
   ],
   "source": [
    "def generate_tsp_data():\n",
    "    '''\n",
    "    Generate tsp data\n",
    "    \n",
    "    Return:\n",
    "    tsp_data (tensor): shape (1, tsp_num, input_dim)\n",
    "    '''\n",
    "    tsp_data = torch.FloatTensor(tsp_num, input_dim).uniform_(0, 1)\n",
    "    return tsp_data.unsqueeze(0)\n",
    "\n",
    "train_dataset = [generate_tsp_data() for _ in range(train_size)]\n",
    "validation_dataset = [generate_tsp_data() for _ in range(validation_size)]\n",
    "print(\"train dataset size: \", len(train_dataset))\n",
    "print(\"validation dataset size: \", len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "class Train(object):\n",
    "    \n",
    "    def __init__(self, model, train_set, validation_set, batch_size = 128, max_grad_norm = 1., lr = 1e-4, update_steps = 5000):\n",
    "        self.model = model\n",
    "        self.train_set = train_set\n",
    "        self.validation_set = validation_set\n",
    "        self.batch_size = batch_size\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.optimizer_all = Adam(list(model.critic.parameters()) + list(model.pointer_net.parameters()), lr = lr)\n",
    "        self.optimizer_pointer = Adam(model.pointer_net.parameters(), lr = lr)\n",
    "        self.lr_scheduler_pointer = lr_scheduler.MultiStepLR(self.optimizer_pointer, \\\n",
    "                                                     list(range(update_steps, update_steps * 1000, update_steps)), gamma=0.96)\n",
    "        self.lr_scheduler_all = lr_scheduler.MultiStepLR(self.optimizer_all, \\\n",
    "                                                     list(range(update_steps, update_steps * 1000, update_steps)), gamma=0.96)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.input_dim = 2 # points dimension\n",
    "        \n",
    "        self.train_rewards = []\n",
    "        self.val_rewards = []\n",
    "    \n",
    "    def train_and_validation(self, n_epoch, training_steps, use_critic = True):\n",
    "        moving_average = 0\n",
    "        for epoch in range(n_epoch):\n",
    "            for step in range(training_steps):\n",
    "                training_set = random.sample(self.train_set, batch_size)\n",
    "                training_set = Variable(torch.cat(training_set).view(self.batch_size, -1, self.input_dim))\n",
    "                L, log_probs, pi, index_list, b = self.model(training_set)\n",
    "\n",
    "                log_probs = log_probs.view(-1)\n",
    "                log_probs[(log_probs < -1000).detach()] = 0.\n",
    "                \n",
    "                if not use_critic:\n",
    "                    if step == 0:\n",
    "                        moving_average = L.mean()\n",
    "                    else:\n",
    "                        moving_average = (moving_average * beta) + ((1. - beta) * L.mean())\n",
    "\n",
    "                    advantage = L - moving_average             \n",
    "                    actor_loss = (advantage * log_probs).mean()\n",
    "                    \n",
    "                                    \n",
    "                    self.optimizer_pointer.zero_grad()\n",
    "                    actor_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.pointer_net.parameters(), self.max_grad_norm, norm_type=2)\n",
    "                    self.optimizer_pointer.step()\n",
    "                    self.lr_scheduler_pointer.step()\n",
    "                    moving_average = moving_average.detach()\n",
    "                         \n",
    "                else:\n",
    "                    critic_loss = self.mse_loss(b.view(-1), L)\n",
    "                    advantage = L - b.view(-1)\n",
    "                    actor_loss = (advantage * log_probs).mean()\n",
    "                    loss = actor_loss + critic_loss\n",
    "                    self.optimizer_all.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(list(self.model.critic.parameters()) + \\\n",
    "                                                   list(self.model.pointer_net.parameters()), self.max_grad_norm, norm_type=2)\n",
    "#                     torch.nn.utils.clip_grad_norm_(, self.max_grad_norm, norm_type=2)\n",
    "                    self.optimizer_all.step()\n",
    "                    self.lr_scheduler_all.step()\n",
    "\n",
    "                self.train_rewards.append(L.mean().data[0])\n",
    "                \n",
    "                if step % 10 == 0:\n",
    "                    self.plot(epoch)\n",
    "                if step % 100 == 0:\n",
    "                    val_set = Variable(torch.cat(self.validation_set).view(len(self.validation_set), -1, self.input_dim))\n",
    "                    L, log_probs, pi, index_list, b = self.model(val_set)\n",
    "                    self.val_rewards.append(L.mean().data[0])  \n",
    "                    \n",
    "                # model save\n",
    "                if step % 1000 == 0:\n",
    "                    if use_critic:\n",
    "                        torch.save(self.model, os.path.join(os.getcwd(), \\\n",
    "                                                            'model_tsp{}_critic.pt'.format(self.model.seq_len)))\n",
    "                    else:\n",
    "                        torch.save(self.model, os.path.join(os.getcwd(), \\\n",
    "                                                            'model_tsp{}_mvg_avg.pt'.format(self.model.seq_len)))\n",
    "    \n",
    "    def plot(self, epoch):\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20,5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('train tour length: epoch %s reward %s' % (epoch, self.train_rewards[-1] if len(self.train_rewards) else 'collecting'))\n",
    "        plt.plot(self.train_rewards)\n",
    "        plt.grid()\n",
    "        plt.subplot(132)\n",
    "        plt.title('val tour length: epoch %s reward %s' % (epoch, self.val_rewards[-1] if len(self.val_rewards) else 'collecting'))\n",
    "        plt.plot(self.val_rewards)\n",
    "        plt.grid()\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving average reward training, more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hexh/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "masked_fill_ only supports boolean masks, but got mask with dtype unsigned char",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     moving_average_model  \u001b[38;5;241m=\u001b[39m moving_average_model \u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      5\u001b[0m train \u001b[38;5;241m=\u001b[39m Train(moving_average_model , train_dataset, validation_dataset, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_critic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m, in \u001b[0;36mTrain.train_and_validation\u001b[0;34m(self, n_epoch, training_steps, use_critic)\u001b[0m\n\u001b[1;32m     26\u001b[0m training_set \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set, batch_size)\n\u001b[1;32m     27\u001b[0m training_set \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mcat(training_set)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim))\n\u001b[0;32m---> 28\u001b[0m L, log_probs, pi, index_list, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m log_probs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m log_probs[(log_probs \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    x (batch_size, seq_len, input_dim)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     prob_list, index_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpointer_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(x)\n\u001b[1;32m     18\u001b[0m     pi \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m, in \u001b[0;36mPointerNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m hidden \u001b[38;5;241m=\u001b[39m (encoder_ht\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), encoder_ct\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m# (batch_size, hidden_dim)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_input\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m prob_list, index_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prob_list, index_list\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 56\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, decoder_input, embedding_x, hidden, encoder_output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_glimpse):\n\u001b[1;32m     55\u001b[0m     logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglimpse(encoder_output, g_l)\n\u001b[0;32m---> 56\u001b[0m     logit, mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask_logit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(logit) \u001b[38;5;66;03m# (batch_size, seq_len)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# ref (batch_size, hidden_dim, seq_len)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mmask_logit\u001b[0;34m(logit, mask, prev_idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     mask_copy[[b \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)], prev_idx\u001b[38;5;241m.\u001b[39mdata] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mlogit\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_copy\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logit, mask_copy\n",
      "\u001b[0;31mRuntimeError\u001b[0m: masked_fill_ only supports boolean masks, but got mask with dtype unsigned char"
     ]
    }
   ],
   "source": [
    "# moving average reward\n",
    "moving_average_model = Model(embedding_dim, hidden_dim, seq_len = tsp_num)\n",
    "if USE_CUDA:\n",
    "    moving_average_model  = moving_average_model .cuda()\n",
    "train = Train(moving_average_model , train_dataset, validation_dataset, lr = 1e-4)\n",
    "train.train_and_validation(6, 10000, use_critic = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor-Critic training, converge more slowly, not stable, which might be worse sometimes.\n",
    "\n",
    "I tried learning rate 1e-3 several times, and it might be explode sometimes. Thus, I used same learning rate 1e-4 as moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "masked_fill_ only supports boolean masks, but got mask with dtype unsigned char",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     ac_model  \u001b[38;5;241m=\u001b[39m ac_model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      5\u001b[0m train \u001b[38;5;241m=\u001b[39m Train(ac_model, train_dataset, validation_dataset, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-4\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_critic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 28\u001b[0m, in \u001b[0;36mTrain.train_and_validation\u001b[0;34m(self, n_epoch, training_steps, use_critic)\u001b[0m\n\u001b[1;32m     26\u001b[0m training_set \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_set, batch_size)\n\u001b[1;32m     27\u001b[0m training_set \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mcat(training_set)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dim))\n\u001b[0;32m---> 28\u001b[0m L, log_probs, pi, index_list, b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m log_probs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m log_probs[(log_probs \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    x (batch_size, seq_len, input_dim)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     prob_list, index_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpointer_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(x)\n\u001b[1;32m     18\u001b[0m     pi \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m, in \u001b[0;36mPointerNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m hidden \u001b[38;5;241m=\u001b[39m (encoder_ht\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m), encoder_ct\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m# (batch_size, hidden_dim)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_input\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(batch_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m prob_list, index_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prob_list, index_list\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/marl/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 56\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, decoder_input, embedding_x, hidden, encoder_output)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_glimpse):\n\u001b[1;32m     55\u001b[0m     logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglimpse(encoder_output, g_l)\n\u001b[0;32m---> 56\u001b[0m     logit, mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask_logit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(logit) \u001b[38;5;66;03m# (batch_size, seq_len)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# ref (batch_size, hidden_dim, seq_len)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mmask_logit\u001b[0;34m(logit, mask, prev_idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prev_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     mask_copy[[b \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size)], prev_idx\u001b[38;5;241m.\u001b[39mdata] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mlogit\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_copy\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m logit, mask_copy\n",
      "\u001b[0;31mRuntimeError\u001b[0m: masked_fill_ only supports boolean masks, but got mask with dtype unsigned char"
     ]
    }
   ],
   "source": [
    "# use critic\n",
    "ac_model = Model(embedding_dim, hidden_dim, seq_len = tsp_num)\n",
    "if USE_CUDA:\n",
    "    ac_model  = ac_model.cuda()\n",
    "train = Train(ac_model, train_dataset, validation_dataset, lr = 1e-4)\n",
    "train.train_and_validation(6, 10000, use_critic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
