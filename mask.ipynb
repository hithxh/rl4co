{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hithx\\miniconda3\\envs\\rl4co\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch import einsum\n",
    "from typing import Optional\n",
    "from einops import  reduce\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CategoricalMasked(Categorical):\n",
    "    def __init__(self, logits: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        self.mask = mask\n",
    "        self.batch, self.nb_action = logits.size()\n",
    "        if mask is None:\n",
    "            super(CategoricalMasked, self).__init__(logits=logits)\n",
    "        else:\n",
    "            self.mask_value = torch.tensor(\n",
    "                torch.finfo(logits.dtype).min, dtype=logits.dtype\n",
    "            )\n",
    "            logits = torch.where(self.mask, logits, self.mask_value)\n",
    "            super(CategoricalMasked, self).__init__(logits=logits)\n",
    "\n",
    "    def entropy(self):\n",
    "        if self.mask is None:\n",
    "            return super().entropy()\n",
    "        # Elementwise multiplication\n",
    "        p_log_p = einsum(\"ij,ij->ij\", self.logits, self.probs)\n",
    "        # Compute the entropy with possible action only\n",
    "        p_log_p = torch.where(\n",
    "            self.mask,\n",
    "            p_log_p,\n",
    "            torch.tensor(0, dtype=p_log_p.dtype, device=p_log_p.device),\n",
    "        )\n",
    "        return -reduce(p_log_p, \"b a -> b\", \"sum\", b=self.batch, a=self.nb_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9309, -1.1033,  0.3168]], requires_grad=True)\n",
      "tensor([[ True,  True, False]])\n",
      "tensor([[0.5981, 0.0782, 0.3237]], grad_fn=<SoftmaxBackward0>) tensor([0.8719], grad_fn=<NegBackward0>)\n",
      "tensor([[0.8843, 0.1157, 0.0000]], grad_fn=<SoftmaxBackward0>) tensor([0.3582], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "logits = torch.randn((1, 3), requires_grad=True) # batch size, nb action\n",
    "print(logits) \n",
    "mask = torch.zeros((1, 3), dtype=torch.bool) # batch size, nb action\n",
    "\n",
    "mask[0, 0] = True\n",
    "mask[0, 1] = True\n",
    "print(mask)\n",
    "action_dist = CategoricalMasked(logits=logits)\n",
    "print(action_dist.probs,action_dist.entropy())\n",
    "action_dist_masked = CategoricalMasked(logits=logits, mask=mask)\n",
    "print(action_dist_masked.probs,action_dist_masked.entropy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_action(self, x, action, mask):\n",
    "        action_dist = self.forward(x)\n",
    "        action_dist_masked = CategoricalMasked(logits=action_dist, mask=mask)\n",
    "        return action_dist_masked.probs, action_dist_masked.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, use_tanh=False, C=10, use_cuda=USE_CUDA):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.use_tanh = use_tanh\n",
    "        self.W_query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_ref   = nn.Conv1d(hidden_size, hidden_size, 1, 1)\n",
    "        self.C = C\n",
    "        \n",
    "        V = torch.FloatTensor(hidden_size)\n",
    "        if use_cuda:\n",
    "            V = V.cuda()  \n",
    "        self.V = nn.Parameter(V)\n",
    "        self.V.data.uniform_(-(1. / math.sqrt(hidden_size)) , 1. / math.sqrt(hidden_size))\n",
    "        \n",
    "    def forward(self, query, ref):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            query: [batch_size x hidden_size]\n",
    "            ref:   ]batch_size x seq_len x hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = ref.size(0)\n",
    "        seq_len    = ref.size(1)\n",
    "\n",
    "        ref = ref.permute(0, 2, 1)\n",
    "        query = self.W_query(query).unsqueeze(2)  # [batch_size x hidden_size x 1]\n",
    "        ref   = self.W_ref(ref)  # [batch_size x hidden_size x seq_len] \n",
    "\n",
    "        expanded_query = query.repeat(1, 1, seq_len) # [batch_size x hidden_size x seq_len]\n",
    "        V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1) # [batch_size x 1 x hidden_size]\n",
    "\n",
    "        logits = torch.bmm(V, F.tanh(expanded_query + ref)).squeeze(1)\n",
    "        \n",
    "        if self.use_tanh:\n",
    "            logits = self.C * F.tanh(logits)\n",
    "        else:\n",
    "            logits = logits  \n",
    "        return ref, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerNet(nn.Module):\n",
    "    def __init__(self, \n",
    "            embedding_size,\n",
    "            hidden_size,\n",
    "            seq_len,\n",
    "            n_glimpses,\n",
    "            tanh_exploration,\n",
    "            use_tanh,\n",
    "            use_cuda=USE_CUDA):\n",
    "        super(PointerNet, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size    = hidden_size\n",
    "        self.n_glimpses     = n_glimpses\n",
    "        self.seq_len        = seq_len\n",
    "        self.use_cuda       = use_cuda\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(seq_len, embedding_size)\n",
    "        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)\n",
    "        self.pointer = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, use_cuda=use_cuda)\n",
    "        self.glimpse = Attention(hidden_size, use_tanh=False, use_cuda=use_cuda)\n",
    "        \n",
    "        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))\n",
    "        self.decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))\n",
    "        \n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def apply_mask_to_logits(self, logits, mask, idxs): \n",
    "        batch_size = logits.size(0)\n",
    "        clone_mask = mask.clone()\n",
    "\n",
    "        if idxs is not None:\n",
    "            clone_mask[[i for i in range(batch_size)], idxs.data] = 1\n",
    "            logits[clone_mask] = -np.inf\n",
    "        return logits, clone_mask\n",
    "            \n",
    "    def forward(self, inputs, target):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            inputs: [batch_size x sourceL]\n",
    "        \"\"\"\n",
    "        batch_size = inputs.size(0)\n",
    "        seq_len    = inputs.size(1)\n",
    "        assert seq_len == self.seq_len\n",
    "        \n",
    "        embedded = self.embedding(inputs)\n",
    "        target_embedded = self.embedding(target)\n",
    "        encoder_outputs, (hidden, context) = self.encoder(embedded)\n",
    "        \n",
    "        mask = torch.zeros(batch_size, seq_len).byte()\n",
    "        if self.use_cuda:\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "        idxs = None\n",
    "       \n",
    "        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            \n",
    "            \n",
    "            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))\n",
    "            \n",
    "            query = hidden.squeeze(0)\n",
    "            for i in range(self.n_glimpses):\n",
    "                ref, logits = self.glimpse(query, encoder_outputs)\n",
    "                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2) \n",
    "                \n",
    "                \n",
    "            _, logits = self.pointer(query, encoder_outputs)\n",
    "            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)\n",
    "            \n",
    "            decoder_input = target_embedded[:,i,:]\n",
    "            \n",
    "            loss += self.criterion(logits, target[:,i])\n",
    "            \n",
    "            \n",
    "        return loss / seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer = PointerNet(embedding_size=32, hidden_size=32, seq_len=10, n_glimpses=1, tanh_exploration=10, use_tanh=True)\n",
    "adam = optim.Adam(pointer.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "train_loss = []\n",
    "val_loss   = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for batch_id, sample_batch in enumerate(train_loader):\n",
    "\n",
    "        inputs = Variable(sample_batch)\n",
    "        target = Variable(torch.sort(sample_batch)[0])\n",
    "        if USE_CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        loss = pointer(inputs, target)\n",
    "\n",
    "        adam.zero_grad()\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        \n",
    "        train_loss.append(loss.data[0])\n",
    "\n",
    "        if batch_id % 10 == 0:\n",
    "\n",
    "            clear_output(True)\n",
    "            plt.figure(figsize=(20,5))\n",
    "            plt.subplot(131)\n",
    "            plt.title('train epoch %s loss %s' % (epoch, train_loss[-1] if len(train_loss) else 'collecting'))\n",
    "            plt.plot(train_loss)\n",
    "            plt.grid()\n",
    "            plt.subplot(132)\n",
    "            plt.title('val epoch %s loss %s' % (epoch, val_loss[-1] if len(val_loss) else 'collecting'))\n",
    "            plt.plot(val_loss)\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "        \n",
    "        if batch_id % 100 == 0:\n",
    "            pointer.eval()\n",
    "            for val_batch in val_loader:\n",
    "                inputs = Variable(val_batch)\n",
    "                target = Variable(torch.sort(val_batch)[0])\n",
    "                if USE_CUDA:\n",
    "                    inputs = inputs.cuda()\n",
    "                    target = target.cuda()\n",
    "\n",
    "                loss = pointer(inputs, target)\n",
    "                val_loss.append(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def attention(q, k, v, d_k, mask=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) /  math.sqrt(d_k)\n",
    "    \n",
    "    # if mask is not None:\n",
    "    #     mask = mask.unsqueeze(1)\n",
    "    #     scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl4co",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
